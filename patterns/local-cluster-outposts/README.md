# Local Cluster on AWS Outposts

This pattern demonstrates how to provision an EKS cluster on AWS Outposts. The solution is comprised of primarily of the following components:

1. A self-managed node group is required to launch instances on the AWS Outposts rack.
2. EKS Clusters on AWS Outpost (local mode) do not support public EKS endpoints. Therefore, configuration to create a standalone instance on the AWS Outposts rack has been provided in order to deploy the pattern from that remote host.
3. An EBS GP2 storage class is required when deploying applications on the EKS cluster.

<b>Links:</b>

- [Amazon EKS on-premises with AWS Outposts](https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html)

## Code

```terraform hl_lines="19-22 46-61 72-88"
{% include  "../../patterns/local-cluster-outposts/eks.tf" %}
```

## Deploy

!!! warning
    Access to an AWS Outposts rack is required to deploy this pattern.

See [here](https://aws-ia.github.io/terraform-aws-eks-blueprints/getting-started/#prerequisites) for the prerequisites and steps to deploy this pattern.

EKS local clusters on AWS Outposts do not support public endpoints. Therefore, the cluster can only be accessed from within the VPC where the Outposts is deployed. The example below demonstrates how to deploy a local cluster on AWS Outpost.

!!! info
    If you already have access to the Outposts rack network (VPN, etc.), you can skip steps 1 and 2:

1. Deploy the remote host where the cluster will be provisioned from:

    ```sh
    cd prerequisites
    terraform init
    terraform apply --auto-approve
    ```

2. If provisioning using the remote host deployed in step 1, connect to the remote host using SSM. You can use the output generated by step 1 to connect:

    !!! note
        You will need to have the [SSM plugin for the AWS CLI installed](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html)

    ```sh
    aws ssm start-session --region <REGION> --target <INSTANCE_ID>
    ```

3. Once connected to the remote host, navigate to where the configuration files have been downloaded and deploy the pattern:

    ```sh
    cd
    terraform init
    terraform apply TF_VAR_outpost_arn=<YOUR-OUTPOST-ARN> --auto-approve
    ```

## Destroy

To remove the resources that were created, the destroy steps need to be executed in the reverse order of the deployment steps:

1. Log back into the remote host using SSM:

    ```sh
    aws ssm start-session --region <REGION> --target <INSTANCE_ID>
    ```

2. Once connected to the remote host, navigate to where the configuration files have been downloaded and deprovision the pattern:

    ```sh
    cd
    terraform destroy --auto-approved
    ```

3. Exit the remote host and navigate to the local `prerequisites/` directory to deprovision the remote host:

    ```sh
    cd prerequisites
    terraform destroy --auto-approved
    ```
